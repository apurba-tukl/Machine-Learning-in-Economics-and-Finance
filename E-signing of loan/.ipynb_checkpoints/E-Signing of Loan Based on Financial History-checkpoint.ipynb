{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>E-Signing of Loan Based on Financial History</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"img/logo.jpg\" width=\"900\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>ESignature:</h4>\n",
    "<br>\n",
    "<p style=\"text-indent:5em\">Electronic signatures aren’t exactly a novelty. They have been around since the American Civil War, during which contracts were signed through Morse. In a modern setting, an e-Sign refers to a unique, digitised, encrypted personal identifier. This is, in essence, different from the ‘wet’ signatures created by hand. The e-Sign is meant to complete transactions, loops, and agreements electronically.</p>\n",
    "\n",
    "<p style=\"text-indent:5em\">In India, the e-Sign has been granted legal status by amendments to various laws, namely the Information Technology Act, Indian Evidence Act and the Negotiable Instruments Act. Early adopters in the financial sector have started using e-Sign to get customers to sign loan and card applications, and loan agreements.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing library for reading, writing and perform basic operations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Importing library for Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#Importing the library for evaluating the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"financial_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the count of each class of dependent variable\n",
    "print(sum(df[\"e_signed\"]==1))\n",
    "print(sum(df[\"e_signed\"]==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for percentage of missing data in each column\n",
    "percent_missing = df.isnull().sum() * 100 / len(df)                \n",
    "missing_value_df = pd.DataFrame({'percent_missing': percent_missing})          \n",
    "\n",
    "missing_value_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-indent:5em\">We see that the data is not highly imbalanced since the ratio between the count of two classes of dependent variable is not so high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the unnecessary columns\n",
    "dataset2 = df.drop(columns = ['entry_id', 'pay_schedule', 'e_signed'])\n",
    "\n",
    "fig = plt.figure(figsize=(15, 12))\n",
    "\n",
    "plt.suptitle('Histograms of Numerical Columns', fontsize=20)\n",
    "for i in range(dataset2.shape[1]):\n",
    "    plt.subplot(6, 3, i + 1)\n",
    "    f = plt.gca()\n",
    "    f.set_title(dataset2.columns.values[i])\n",
    "\n",
    "    vals = np.size(dataset2.iloc[:, i].unique())\n",
    "    if vals >= 100:\n",
    "        vals = 100\n",
    "\n",
    "    plt.hist(dataset2.iloc[:, i], bins=vals, color='#3F5D7D')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation of independent variable with dependent variable\n",
    "\n",
    "dataset2.corrwith(df.e_signed).plot.bar(figsize=(20,10),title=\"correlation wigh E signed\",\n",
    "                                             fontsize=20,rot=45,grid= True,color=['pink','green',\n",
    "                                                                                  'blue','cyan','magenta'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div>\n",
    "<img src=\"img/ann.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiatizig into new variable and creating the dummy variable for categorical data\n",
    "data=df\n",
    "dummy=pd.get_dummies(data[\"pay_schedule\"])\n",
    "dummy=dummy.drop(labels=[\"bi-weekly\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data=data.drop([\"pay_schedule\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data=pd.concat([data,dummy],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating the data into dependent and independent varioable. Response contains the esigned column which needs to be computed\n",
    "#from the dataset\n",
    "response = data[\"e_signed\"]\n",
    "dataset = data.drop(columns = [\"e_signed\", \"entry_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming the data using StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X= StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into Train and Test Set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset,\n",
    "                                                    response,\n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting and transforming our data\n",
    "X_train=sc_X.fit_transform(X_train)\n",
    "X_test=sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building ANN Using Keras\n",
    "\n",
    "<img src=\"img/keras.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the layers in ANN\n",
    "classifier.add(Dense(units = 10, kernel_initializer = 'uniform', activation = 'relu', input_dim = 21))\n",
    "\n",
    "\n",
    "classifier.add(Dense(units = 10, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict functioon the test data using the model\n",
    "pred=classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = (pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "<div>\n",
    "    <br>\n",
    "<img src=\"img/ml.jpg\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AGE\n",
    "\n",
    "We will group our age data as follows:\n",
    "<div>\n",
    "    <br>\n",
    "<img src=\"img/age_group.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the Empty List\n",
    "AGE=[]\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if(df[\"age\"][i]<=45):\n",
    "        AGE.append(\"YOUTH\")\n",
    "        \n",
    "    else:\n",
    "        AGE.append(\"SENIOR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Months Employed and Personal Account\n",
    "\n",
    "We will calculate months employed as follows:\n",
    "<br>\n",
    "<p style=\"text-indent:5em\">month employed=month employed+ years employed*12</p>\n",
    "    <p style=\"text-indent:5em\">account_m=account_m+account_y*12</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMPLOYED=[]\n",
    "for i in range(len(df)):\n",
    "    x=df[\"months_employed\"][i]+df[\"years_employed\"][i]*12\n",
    "    EMPLOYED.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PA=[]\n",
    "for i in range(len(df)):\n",
    "    x=df[\"personal_account_m\"][i]+df[\"personal_account_y\"][i]*12\n",
    "    PA.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_risk=[]\n",
    "for i in range(len(df)):\n",
    "    x=(df[\"risk_score_2\"][i]+df[\"risk_score_3\"][i]+df[\"risk_score_4\"][i]+df[\"risk_score_5\"][i])/4\n",
    "    avg_risk.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_quality=[]\n",
    "for i in range(len(df)):\n",
    "    x=(df[\"ext_quality_score_2\"][i]+df[\"ext_quality_score\"][i])/2\n",
    "    ext_quality.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging all the features into Single Dataframe\n",
    "\n",
    "<p style=\"text-indent:5em\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting all lists into DataFrame\n",
    "AGE=pd.DataFrame(AGE)\n",
    "EMPLOYED=pd.DataFrame(EMPLOYED)\n",
    "PA=pd.DataFrame(PA)\n",
    "avg_risk=pd.DataFrame(avg_risk)\n",
    "ext_quality=pd.DataFrame(ext_quality)\n",
    "\n",
    "#Concatng all the features\n",
    "featured=pd.concat([AGE,EMPLOYED,PA,avg_risk,ext_quality],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featured.columns=[\"AGE\",\"EMPLOYED\",\"PA\",\"RISK\",\"QUALITY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featured.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Dummy Variable\n",
    "dummy1=pd.get_dummies(featured[\"AGE\"])\n",
    "dummy1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the AGE Variable since dummy variable is created\n",
    "featured=featured.drop([\"AGE\"],axis=1)\n",
    "\n",
    "#Concating the data and dummy variable\n",
    "featured=pd.concat([featured,dummy1],axis=1)\n",
    "featured.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifying the Data into dependent and independent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependent Variable\n",
    "dep=\"e_signed\"\n",
    "\n",
    "#Selelcting all the column as independent variable\n",
    "ind=df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the dependent variable from independent\n",
    "ind.remove(dep)\n",
    "ind.remove(\"entry_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the Data\n",
    "X=df[ind]\n",
    "Y=df[dep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concating the featured data and unfeatured data\n",
    "X=pd.concat([X,featured],axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the unnecessay data from our dataset\n",
    "X=X.drop(labels=[\"age\",\"months_employed\",\"years_employed\",\"personal_account_m\",\"personal_account_y\",\"risk_score_2\",\"risk_score_3\"\n",
    "                ,\"risk_score_4\",\"risk_score_5\",\"ext_quality_score_2\",\"ext_quality_score\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Dummy Variable\n",
    "dummy2=pd.get_dummies(X[\"pay_schedule\"])\n",
    "\n",
    "#Removing the trap\n",
    "dummy2=dummy2.drop(labels=[\"bi-weekly\"],axis=1)\n",
    "dummy2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the pay_schedule since dummy variable is created\n",
    "X=X.drop(labels=[\"pay_schedule\"],axis=1)\n",
    "\n",
    "#Concating the data and dummy variable\n",
    "X=pd.concat([X,dummy2],axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Transformation and Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale=StandardScaler()\n",
    "#Transforming the data\n",
    "X=scale.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(X,Y,test_size=0.25,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the object\n",
    "model=RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <br>\n",
    "<img src=\"img/random.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model\n",
    "model.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions on test data\n",
    "pred=model.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ytest,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div>\n",
    "    <br>\n",
    "<img src=\"img/gradient.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d\">Read More</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "for learning_rate in lr_list:\n",
    "    gb_clf = GradientBoostingClassifier(n_estimators=20, learning_rate=learning_rate, max_features=2, max_depth=2, random_state=0)\n",
    "    gb_clf.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learning rate: \", learning_rate)\n",
    "print(\"Accuracy score (training): {0:.3f}\".format(gb_clf.score(xtrain, ytrain)))\n",
    "print(\"Accuracy score (validation): {0:.3f}\".format(gb_clf.score(xtest, ytest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XgBoost\n",
    "\n",
    "\n",
    "<div>\n",
    "    <br>\n",
    "<img src=\"img/xg_boost.jpeg\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing xgboost\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the model\n",
    "xgb_clf = XGBClassifier()\n",
    "xgb_clf.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = xgb_clf.score(xtest, ytest)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine\n",
    "\n",
    "\n",
    "<br>\n",
    "<img src=\"img/sv.jpg\" height=100px >\n",
    "  \n",
    "  \n",
    "<br>\n",
    "<p>For more reading click <a href=\"https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/\">here</a>.<br><br>\n",
    "    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(random_state = 0, kernel = 'rbf')\n",
    "classifier.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting Test Set\n",
    "y_pred = classifier.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the Accuracy Score\n",
    "acc = accuracy_score(ytest, y_pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "<h4>Models implemented:</h4>\n",
    "\n",
    "<ul>\n",
    "    <li>Artificial Neural Network</li>\n",
    "    <li>Random Forest Classifier</li>\n",
    "    <li>Gradient Boosting</li>\n",
    "    <li>Support Vector Machine</li>\n",
    "    <li>Xg Boost</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><u>Conclusion</u><h2></center\n",
    "    \n",
    " <ul>\n",
    "    <li>XgBoost Algorithm performs the best and give the accuracy of 62 %</li>\n",
    "    <li>We see that the ANN with no feature engineering performs far better than SVM, Random Forest with feature engineering</li>\n",
    "    <li>Though we didnt get very high accuracy but this can help the banks in knowing whether the customer is risky or not.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
